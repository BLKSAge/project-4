{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b212f146-aea7-443a-a3ac-e6c91f4eaf3d",
   "metadata": {},
   "source": [
    "---\n",
    "## This is a test of the emergency throw stuff at the wall service\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e9822-cfea-42fa-804b-77e483b4eabf",
   "metadata": {},
   "source": [
    "## Map of stuff included in this notebook at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b6aa06-f1ed-4c3b-ac19-bc7d9fedaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "# core just in case\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ML framework (TensorFlow -> includes Keras)\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tensorflow.keras import layers, Model, callbacks, optimizers\n",
    "\n",
    "# Hugging Face * had a small error with transformers -> (pip install transformers datasets huggingface_hub) -> error go byebye.\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    pipeline,)\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Evaluation / Utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Logging / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Note had to install earlier version of Keras via (pip install tf-keras) -> restart kernel -> working:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede54d59-a944-4717-8655-ac6b24762cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                          statement   status\n",
      "0           0                                         oh my gosh  Anxiety\n",
      "1           1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
      "2           2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
      "3           3  I've shifted my focus to something else but I'...  Anxiety\n",
      "4           4  I'm restless and restless, it's been a month n...  Anxiety\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/sentiment.csv')  \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f12d3ef0-d0c7-4800-99dd-29dcc2fb273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status\n",
      "Normal                  16351\n",
      "Depression              15404\n",
      "Suicidal                10653\n",
      "Anxiety                  3888\n",
      "Bipolar                  2877\n",
      "Stress                   2669\n",
      "Personality disorder     1201\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03f2b6d-e4ed-4aef-a110-1a1473de11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Leaning on bert instead of installing SpaCy just yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b8fcd84-d857-4b6b-ae74-1ebffbc02958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class Definition\n",
    "class HF_Dataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        # tokenize all at once\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c8f678-574d-4dfb-9121-04f3c0f4801f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Prep datasets\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_dataset = HF_Dataset(\u001b[43mtrain_texts\u001b[49m, train_labels, tokenizer)\n\u001b[32m      3\u001b[39m test_dataset  = HF_Dataset(test_texts,  test_labels,  tokenizer)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_texts' is not defined"
     ]
    }
   ],
   "source": [
    "# Prep datasets\n",
    "train_dataset = HF_Dataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset  = HF_Dataset(test_texts,  test_labels,  tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6a2475-8667-4902-9ab9-cff626188e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping: {'Anxiety': 0, 'Bipolar': 1, 'Depression': 2, 'Normal': 3, 'Personality disorder': 4, 'Stress': 5, 'Suicidal': 6}\n"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df[\"label\"] = le.fit_transform(df[\"status\"])\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Mapping:\", label_mapping)\n",
    "num_labels = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0fb68f7-e39b-4c2f-ac25-d9dc8c36ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 47738\n",
      "Validation size: 5305\n",
      "Label distribution (train):\n",
      "status\n",
      "Normal                  0.308266\n",
      "Depression              0.290398\n",
      "Suicidal                0.200846\n",
      "Anxiety                 0.073296\n",
      "Bipolar                 0.054234\n",
      "Stress                  0.050316\n",
      "Personality disorder    0.022644\n",
      "Name: proportion, dtype: float64\n",
      "Label distribution (test):\n",
      "status\n",
      "Normal                  0.308200\n",
      "Depression              0.290481\n",
      "Suicidal                0.200754\n",
      "Anxiety                 0.073327\n",
      "Bipolar                 0.054288\n",
      "Stress                  0.050330\n",
      "Personality disorder    0.022620\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.1, stratify=df[\"label\"], random_state=42)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Validation size:\", len(test_df))\n",
    "print(\"Label distribution (train):\")\n",
    "print(train_df[\"status\"].value_counts(normalize=True))\n",
    "print(\"Label distribution (test):\")\n",
    "print(test_df[\"status\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9539ab5b-4839-4a2c-9391-0e996f8d9d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode(df_subset):\n",
    "    # force strings and drop/fill NaNs\n",
    "    texts = df_subset[\"statement\"].astype(str).fillna(\"\").tolist()\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\",\n",
    "    )\n",
    "    labels = tf.constant(df_subset[\"label\"].values)\n",
    "    return encodings, labels\n",
    "\n",
    "train_enc, train_labels = encode(train_df)\n",
    "test_enc,   test_labels   = encode(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9f1a71-ee6d-49d2-bc13-005dcca364d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tf.data pipelines\n",
    "train_ds = (\n",
    "    tf.data.Dataset\n",
    "      .from_tensor_slices((dict(train_enc), train_labels))\n",
    "      .shuffle(2000)\n",
    "      .batch(16)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset\n",
    "      .from_tensor_slices((dict(test_enc), test_labels))\n",
    "      .batch(16)\n",
    "      .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b994d2-0d0f-42a0-b40c-dfda5ddcd5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute '_distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load & compile model\u001b[39;00m\n\u001b[32m      2\u001b[39m model = TFAutoModelForSequenceClassification.from_pretrained(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     num_labels=num_labels\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSparseCategoricalCrossentropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrecall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_tf_utils.py:1547\u001b[39m, in \u001b[36mTFPreTrainedModel.compile\u001b[39m\u001b[34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;66;03m# This argument got renamed, we need to support both versions\u001b[39;00m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msteps_per_execution\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parent_args:\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1548\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweighted_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1554\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1555\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1558\u001b[39m     \u001b[38;5;28msuper\u001b[39m().compile(\n\u001b[32m   1559\u001b[39m         optimizer=optimizer,\n\u001b[32m   1560\u001b[39m         loss=loss,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m         **kwargs,\n\u001b[32m   1567\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4021\u001b[39m, in \u001b[36m_DefaultDistributionExtended.variable_created_in_scope\u001b[39m\u001b[34m(self, v)\u001b[39m\n\u001b[32m   4020\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvariable_created_in_scope\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[32m-> \u001b[39m\u001b[32m4021\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_distribute_strategy\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Variable' object has no attribute '_distribute_strategy'"
     ]
    }
   ],
   "source": [
    "# Load & compile model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.Recall(name=\"recall\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c9d4e-46ff-4237-b8b7-d607053a5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1425c-9fe0-45a8-a792-9f8818c067f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdb604-db3b-40c2-a71c-c41b06dab55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b7786-e815-4e8c-b0dc-2f4eed2dd231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
